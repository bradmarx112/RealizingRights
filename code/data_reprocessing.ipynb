{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14102\\Brown\\Realizing_Rights\\RealizingRights\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.web_utils import closest_link_match, make_https, prepend_root_to_url, make_driver_utils\n",
    "from url_scraper.link_data import LinkData\n",
    "from main import main\n",
    "from objects.scrape_lists import blacklist_terms, link_keywords, board_meeting_keywords, social_media_sites\n",
    "from url_scraper.web_scraper import DistrictWebsiteScraper\n",
    "from utilities.data_utils import format_input_district_df, format_output_boe_df\n",
    "\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "import queue\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_info = {'path': 'data/USSchoolDistrictWebsiteInfo.xlsx', 'sheet': 'ELSI Export', 'head_row': 6}\n",
    "# write_file_path = 'data/SampleOutput.csv'\n",
    "    \n",
    "# main(source_info=source_info, write_file_path=write_file_path, verbose=False, max_dist_runs=None, out_file_name=write_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output = pd.read_csv('data/SampleOutput.csv')\n",
    "\n",
    "def check_malformed(url: str) -> bool:\n",
    "    if len([char.start() for char in re.finditer('//', url)]) > 1:\n",
    "        return True\n",
    "    elif 'javascript' in url:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# current_output['Board Meeting URL Link'] = current_output.apply(lambda x: str(x['Board Meeting URL Link']), axis=1)\n",
    "current_pop_output = current_output[~current_output['Board Meeting URL Link'].isna()].reset_index(drop=True)\n",
    "current_unpop_output = current_output[current_output['Board Meeting URL Link'].isna()].reset_index(drop=True)\n",
    "\n",
    "current_pop_output['BadBoardURL'] = current_pop_output.apply(lambda x: check_malformed(url=x['Board Meeting URL Link']), axis=1)\n",
    "malformed_brd_mt_df = current_pop_output[current_pop_output['BadBoardURL']]\n",
    "current_pop_output = current_pop_output[~current_pop_output['BadBoardURL']]\n",
    "current_pop_output.drop('BadBoardURL', axis=1, inplace=True)\n",
    "current_pop_output = current_pop_output.append(current_unpop_output)\n",
    "malformed_brd_mt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output = pd.read_csv('data/SampleOutput.csv')\n",
    "missing_brd_mt_df = current_output[current_output['Board Meeting URL Link'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = queue.Queue()\n",
    "full_df = pd.DataFrame()\n",
    "urls_processed = 0\n",
    "\n",
    "def reprocess_missing_data(source: pd.DataFrame, write_file_path: str, verbose: bool, max_dist_runs: int) -> None:\n",
    "    global full_df\n",
    "\n",
    "    district_df_to_use = source[:max_dist_runs]\n",
    "    # Scrape data\n",
    "    \n",
    "    url_list_to_process = list(district_df_to_use['URL'])\n",
    "    num_urls = len(url_list_to_process)\n",
    "    url_id_zip = [(url, a_id) for url, a_id in zip(list(district_df_to_use['URL']), list(district_df_to_use['District ID']))]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "\n",
    "        _ = executor.submit(build_output_df, num_urls)\n",
    "        executor.map(scrape_district, url_id_zip)\n",
    "\n",
    "    full_df.to_csv(write_file_path, index=False)\n",
    "\n",
    "\n",
    "def scrape_district(url_id_tuple: tuple):\n",
    "    global urls_processed\n",
    "    print(f'Parsing {url_id_tuple[0]}...')\n",
    "    try:\n",
    "        district_scraper = DistrictWebsiteScraper(url=url_id_tuple[0], agency_id=url_id_tuple[1], verbose=False)\n",
    "        district_scraper.find_board_meeting_and_social_media_links()\n",
    "        pipeline.put(district_scraper.url_data)\n",
    "        print(f'{url_id_tuple[0]} Done')\n",
    "    except Exception as e:\n",
    "        print(f\"{url_id_tuple[0]} Error: {e}\")\n",
    "        urls_processed += 1\n",
    "\n",
    "\n",
    "def build_output_df(num_urls: int):\n",
    "    global full_df\n",
    "    global urls_processed\n",
    "    while urls_processed < num_urls:\n",
    "        url_info_dict = pipeline.get()\n",
    "        url_info_row = format_output_boe_df(url_data=url_info_dict, social_media_sites=social_media_sites)\n",
    "        full_df = full_df.append(url_info_row)\n",
    "        urls_processed += 1\n",
    "        print(urls_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_path = 'data/reprocessed.csv'\n",
    "\n",
    "reprocess_missing_data(source=malformed_brd_mt_df, write_file_path=write_file_path, max_dist_runs=9999, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_info = {'path': 'data/USSchoolDistrictWebsiteInfo.xlsx', 'sheet': 'ELSI Export', 'head_row': 6}\n",
    "xlsx_input = pd.read_excel(io=source_info['path'], sheet_name=source_info['sheet'], header=source_info['head_row'])\n",
    "district_df = format_input_district_df(dist_df=xlsx_input)\n",
    "pop_output = current_output[~current_output['Board Meeting URL Link'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_info = district_df.merge(full_df, left_on='Agency ID', right_on='District ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_output_df = pop_output.append(full_df_info)\n",
    "updated_output_df = current_pop_output.append(full_df_info)\n",
    "updated_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_output_df.to_csv('data/SampleOutput.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output = pd.read_csv('data/SampleOutput.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_right_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
